{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "name": "Copy of Part 4 - Fashion-MNIST (Exercises).ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srkhvx/Intelligent-Cyber-Security/blob/master/Copy_of_Part_4_Fashion_MNIST_(Exercises).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcF7mmHxbcKq",
        "colab_type": "text"
      },
      "source": [
        "# Classifying Fashion-MNIST\n",
        "\n",
        "Now it's your turn to build and train a neural network. You'll be using the [Fashion-MNIST dataset](https://github.com/zalandoresearch/fashion-mnist), a drop-in replacement for the MNIST dataset. MNIST is actually quite trivial with neural networks where you can easily achieve better than 97% accuracy. Fashion-MNIST is a set of 28x28 greyscale images of clothes. It's more complex than MNIST, so it's a better representation of the actual performance of your network, and a better representation of datasets you'll use in the real world.\n",
        "\n",
        "<img src='assets/fashion-mnist-sprite.png' width=500px>\n",
        "\n",
        "In this notebook, you'll build your own neural network. For the most part, you could just copy and paste the code from Part 3, but you wouldn't be learning. It's important for you to write the code yourself and get it to work. Feel free to consult the previous notebooks though as you work through this.\n",
        "\n",
        "First off, let's load the dataset through torchvision."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjXB_3q3bcKs",
        "colab_type": "code",
        "outputId": "372f5e88-4485-4496-f175-c2bd6b2793cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "import helper\n",
        "\n",
        "# Define a transform to normalize the data\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5,), (0.5,))])\n",
        "# Download and load the training data\n",
        "trainset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "\n",
        "# Download and load the test data\n",
        "testset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=False, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to /root/.pytorch/F_MNIST_data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "26427392it [00:01, 14837812.87it/s]                             \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting /root/.pytorch/F_MNIST_data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to /root/.pytorch/F_MNIST_data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "32768it [00:00, 103121.47it/s]           \n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting /root/.pytorch/F_MNIST_data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to /root/.pytorch/F_MNIST_data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "4423680it [00:01, 4373536.05it/s]                             \n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting /root/.pytorch/F_MNIST_data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to /root/.pytorch/F_MNIST_data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "8192it [00:00, 34811.56it/s]            "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting /root/.pytorch/F_MNIST_data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n",
            "Processing...\n",
            "Done!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FukJWCQ1bcKu",
        "colab_type": "text"
      },
      "source": [
        "Here we can see one of the images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ft3pgtwybcKv",
        "colab_type": "code",
        "outputId": "67b4e7c5-7c88-469d-9783-42c5efe2267b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        }
      },
      "source": [
        "image, label = next(iter(trainloader))\n",
        "imshow(image[0,:]);"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOsAAADrCAYAAACICmHVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSRRxZgAbcKx",
        "colab_type": "text"
      },
      "source": [
        "## Building the network\n",
        "\n",
        "Here you should define your network. As with MNIST, each image is 28x28 which is a total of 784 pixels, and there are 10 classes. You should include at least one hidden layer. We suggest you use ReLU activations for the layers and to return the logits or log-softmax from the forward pass. It's up to you how many layers you add and the size of those layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3aJN7Rx_bcKx",
        "colab_type": "code",
        "outputId": "c5af0f5e-0451-4c18-9443-14e87680bafd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# TODO: Define your network architecture here\n",
        "from torch import nn\n",
        "net = nn.Sequential(nn.Linear(784, 32),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(32, 256),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(256,10),\n",
        "                      nn.Softmax(dim=1))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(2.3027, grad_fn=<NllLossBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHprNG1GbcKz",
        "colab_type": "text"
      },
      "source": [
        "# Train the network\n",
        "\n",
        "Now you should create your network and train it. First you'll want to define [the criterion](http://pytorch.org/docs/master/nn.html#loss-functions) ( something like `nn.CrossEntropyLoss`) and [the optimizer](http://pytorch.org/docs/master/optim.html) (typically `optim.SGD` or `optim.Adam`).\n",
        "\n",
        "Then write the training code. Remember the training pass is a fairly straightforward process:\n",
        "\n",
        "* Make a forward pass through the network to get the logits \n",
        "* Use the logits to calculate the loss\n",
        "* Perform a backward pass through the network with `loss.backward()` to calculate the gradients\n",
        "* Take a step with the optimizer to update the weights\n",
        "\n",
        "By adjusting the hyperparameters (hidden units, learning rate, etc), you should be able to get the training loss below 0.4."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ks9uCzJwbcKz",
        "colab_type": "code",
        "outputId": "d4929c17-6448-4912-8086-19b1e2f707cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "# TODO: Create the network, define the criterion and optimizer\n",
        "from torch import optim\n",
        "net = nn.Sequential(nn.Linear(784, 32),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(32, 256),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(256,10),\n",
        "                      nn.LogSoftmax(dim=1))\n",
        "\n",
        "\n",
        "criterion = nn.NLLLoss()\n",
        "\n",
        "optimizer=optim.SGD(net.parameters(), lr=0.01)\n",
        "\n",
        "epochs = 5 # what is an epoch?\n",
        "\n",
        "for e in range(epochs):\n",
        "    running_loss = 0\n",
        "    for images, labels in trainloader:\n",
        "      \n",
        "        # Flatten images into a 784 long vector\n",
        "        images = images.view(images.shape[0], -1)\n",
        "    \n",
        "        # reset the gradients of the optimizer\n",
        "        optimizer.zero_grad()\n",
        "        # calucalte the prediciotns\n",
        "        logits = net(images)\n",
        "        # calculate the loss on those prediciotns\n",
        "        loss = criterion(logits, labels)\n",
        "        # calculate the gradients\n",
        "        loss.backward()\n",
        "        # update the weights\n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "    else:\n",
        "        print(f\"Epoch {e+1} Training loss: {running_loss/len(trainloader)}\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Training loss: 0.9122943542341688\n",
            "Epoch 2 Training loss: 0.5340315066039689\n",
            "Epoch 3 Training loss: 0.4751858380335226\n",
            "Epoch 4 Training loss: 0.44260520148061233\n",
            "Epoch 5 Training loss: 0.42291697945549034\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S06jG1287t73",
        "colab_type": "code",
        "outputId": "eab41e72-2a82-43da-9c4f-40508a101a2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        }
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "images, labels = next(iter(testloader))\n",
        "\n",
        "img = images[0].view(1, 784)\n",
        "# Turn off gradients to speed up this part\n",
        "with torch.no_grad():\n",
        "    logps = net(img)\n",
        "\n",
        "# Output of the network are log-probabilities, need to take exponential for probabilities\n",
        "ps = torch.exp(logps)\n",
        "view_classify(img.view(1, 28, 28), ps)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAADjCAYAAADQWoDbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZmZWSm5QZmZWSm5QZmZWSm5QZmZWSv8fo7wZNwPtHQsAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x648 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "giyNMmXlbcK1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torch import nn, optim\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "def test_network(net, trainloader):\n",
        "\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
        "\n",
        "    dataiter = iter(trainloader)\n",
        "    images, labels = dataiter.next()\n",
        "\n",
        "    # Create Variables for the inputs and targets\n",
        "    inputs = Variable(images)\n",
        "    targets = Variable(images)\n",
        "\n",
        "    # Clear the gradients from all Variables\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass, then backward pass, then update weights\n",
        "    output = net.forward(inputs)\n",
        "    loss = criterion(output, targets)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return True\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddTsf0F4NQeq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def imshow(image, ax=None, title=None, normalize=True):\n",
        "    \"\"\"Imshow for Tensor.\"\"\"\n",
        "    if ax is None:\n",
        "        fig, ax = plt.subplots()\n",
        "    image = image.numpy().transpose((1, 2, 0))\n",
        "\n",
        "    if normalize:\n",
        "        mean = np.array([0.485, 0.456, 0.406])\n",
        "        std = np.array([0.229, 0.224, 0.225])\n",
        "        image = std * image + mean\n",
        "        image = np.clip(image, 0, 1)\n",
        "\n",
        "    ax.imshow(image)\n",
        "    ax.spines['top'].set_visible(False)\n",
        "    ax.spines['right'].set_visible(False)\n",
        "    ax.spines['left'].set_visible(False)\n",
        "    ax.spines['bottom'].set_visible(False)\n",
        "    ax.tick_params(axis='both', length=0)\n",
        "    ax.set_xticklabels('')\n",
        "    ax.set_yticklabels('')\n",
        "\n",
        "    return ax\n",
        "\n",
        "\n",
        "def view_recon(img, recon):\n",
        "    ''' Function for displaying an image (as a PyTorch Tensor) and its\n",
        "        reconstruction also a PyTorch Tensor\n",
        "    '''\n",
        "\n",
        "    fig, axes = plt.subplots(ncols=2, sharex=True, sharey=True)\n",
        "    axes[0].imshow(img.numpy().squeeze())\n",
        "    axes[1].imshow(recon.data.numpy().squeeze())\n",
        "    for ax in axes:\n",
        "        ax.axis('off')\n",
        "        ax.set_adjustable('box-forced')\n",
        "\n",
        "def view_classify(img, ps, version=\"MNIST\"):\n",
        "    ''' Function for viewing an image and it's predicted classes.\n",
        "    '''\n",
        "    ps = ps.data.numpy().squeeze()\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(figsize=(6,9), ncols=2)\n",
        "    ax1.imshow(img.resize_(1, 28, 28).numpy().squeeze())\n",
        "    ax1.axis('off')\n",
        "    ax2.barh(np.arange(10), ps)\n",
        "    ax2.set_aspect(0.1)\n",
        "    ax2.set_yticks(np.arange(10))\n",
        "    if version == \"MNIST\":\n",
        "        ax2.set_yticklabels(np.arange(10))\n",
        "    elif version == \"Fashion\":\n",
        "        ax2.set_yticklabels(['T-shirt/top',\n",
        "                            'Trouser',\n",
        "                            'Pullover',\n",
        "                            'Dress',\n",
        "                            'Coat',\n",
        "                            'Sandal',\n",
        "                            'Shirt',\n",
        "                            'Sneaker',\n",
        "                            'Bag',\n",
        "                            'Ankle Boot'], size='small');\n",
        "    ax2.set_title('Class Probability')\n",
        "    ax2.set_xlim(0, 1.1)\n",
        "\n",
        "    plt.tight_layout()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}